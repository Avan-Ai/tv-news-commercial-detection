{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´...\n",
            "ğŸ“„ Reading BBC.txt ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BBC.txt: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BBC.txt: 17720it [00:03, 4843.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Reading CNN.txt ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CNN.txt: 22545it [00:05, 3984.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Reading CNNIBN.txt ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "CNNIBN.txt: 33117it [00:06, 5084.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Reading NDTV.txt ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "NDTV.txt: 17051it [00:03, 4900.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Reading TIMESNOW.txt ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TIMESNOW.txt: 39252it [00:10, 3692.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§¹ Ø­Ø°Ù 3894 ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± ØªÚ©Ø±Ø§Ø±ÛŒ\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\niman\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:58:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ø§Ù†ØªØ®Ø§Ø¨ 43 ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ù‡Ù… (Ø¨Ø±Ø§Ø³Ø§Ø³ threshold = mean)\n",
            "â• ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: 55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\niman\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:58:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Ø§Ù†ØªØ®Ø§Ø¨ 26 ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ù‡Ù… (Ø¨Ø±Ø§Ø³Ø§Ø³ threshold = mean)\n",
            "âœ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯.\n",
            "ğŸ“Š Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡: (129685, 27)\n",
            "ğŸ“ ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: FinalPreProcessing.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§\n",
        "DATA_DIR = '../data'\n",
        "FEATURE_COUNT = 4125\n",
        "\n",
        "# ------------------------------------\n",
        "# ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ LibSVM Ø¨Ù‡ DataFrame\n",
        "# ------------------------------------\n",
        "def parse_libsvm_line(line):\n",
        "    parts = line.strip().split()\n",
        "    label = int(parts[0].replace('+', ''))\n",
        "    features = [0.0] * FEATURE_COUNT\n",
        "    for item in parts[1:]:\n",
        "        index, value = item.split(\":\")\n",
        "        index = int(index)\n",
        "        if 1 <= index <= FEATURE_COUNT:\n",
        "            features[index - 1] = float(value)\n",
        "    return [label] + features\n",
        "\n",
        "def load_all_data():\n",
        "    data = []\n",
        "    for filename in os.listdir(DATA_DIR):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            filepath = os.path.join(DATA_DIR, filename)\n",
        "            print(f\"ğŸ“„ Reading {filename} ...\")\n",
        "            with open(filepath, \"r\") as file:\n",
        "                for line in tqdm(file, desc=filename):\n",
        "                    row = parse_libsvm_line(line)\n",
        "                    data.append(row)\n",
        "    columns = ['label'] + [f'feature_{i}' for i in range(1, FEATURE_COUNT + 1)]\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    return df\n",
        "\n",
        "# ------------------------------------\n",
        "# Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± ØªÚ©Ø±Ø§Ø±ÛŒ\n",
        "# ------------------------------------\n",
        "def remove_single_value_columns(df):\n",
        "    cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
        "    print(f\"ğŸ§¹ Ø­Ø°Ù {len(cols_to_drop)} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± ØªÚ©Ø±Ø§Ø±ÛŒ\")\n",
        "    return df.drop(columns=cols_to_drop)\n",
        "\n",
        "# ------------------------------------\n",
        "# Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§\n",
        "# ------------------------------------\n",
        "def normalize(df):\n",
        "    X = df.drop(columns=['label'])\n",
        "    y = df['label']\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "    return pd.concat([y.reset_index(drop=True), X_scaled_df], axis=1)\n",
        "\n",
        "# ------------------------------------\n",
        "# Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª\n",
        "# ------------------------------------\n",
        "def replace_outliers_with_mean(df):\n",
        "    labels = df['label']\n",
        "    features = df.drop(columns=['label'])\n",
        "    cleaned = features.copy()\n",
        "    for col in cleaned.columns:\n",
        "        Q1 = cleaned[col].quantile(0.25)\n",
        "        Q3 = cleaned[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "        outliers = cleaned[(cleaned[col] < lower) | (cleaned[col] > upper)][col]\n",
        "        if len(outliers) > 0:\n",
        "            if len(outliers) > 1000:\n",
        "                distances = np.maximum(outliers - upper, lower - outliers)\n",
        "                top_outliers = distances.abs().sort_values(ascending=False).head(1000).index\n",
        "            else:\n",
        "                top_outliers = outliers.index\n",
        "            mean_val = cleaned[(cleaned[col] >= lower) & (cleaned[col] <= upper)][col].mean()\n",
        "            cleaned.loc[top_outliers, col] = mean_val\n",
        "    cleaned['label'] = labels\n",
        "    return cleaned\n",
        "\n",
        "# ------------------------------------\n",
        "# Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ø¨Ø§ XGBoost\n",
        "# ------------------------------------\n",
        "def select_important_features(X, y, threshold=\"mean\"):\n",
        "    try:\n",
        "        y = np.array(y)\n",
        "        y = np.where(y == -1, 0, y)\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            feature_names = X.columns\n",
        "            X = X.select_dtypes(include=[np.number])\n",
        "            X_array = X.values\n",
        "        else:\n",
        "            X_array = X\n",
        "            feature_names = [f'feature_{i}' for i in range(X_array.shape[1])]\n",
        "        model = XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1, use_label_encoder=False, eval_metric='logloss')\n",
        "        model.fit(X_array, y)\n",
        "        selector = SelectFromModel(model, threshold=threshold, prefit=True)\n",
        "        X_selected_array = selector.transform(X_array)\n",
        "        if X_selected_array.shape[1] == 0:\n",
        "            print(\"âš ï¸ Ù‡ÛŒÚ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒØ§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯. Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø§ÙˆÙ„ÛŒÙ‡.\")\n",
        "            return pd.DataFrame(X_array, columns=feature_names), y\n",
        "        selected_cols = [col for col, keep in zip(feature_names, selector.get_support()) if keep]\n",
        "        X_selected_df = pd.DataFrame(X_selected_array, columns=selected_cols)\n",
        "        print(f\"âœ… Ø§Ù†ØªØ®Ø§Ø¨ {len(selected_cols)} ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ù‡Ù… (Ø¨Ø±Ø§Ø³Ø§Ø³ threshold = {threshold})\")\n",
        "        return X_selected_df, y\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {e}\")\n",
        "        return pd.DataFrame(X_array, columns=feature_names), y\n",
        "\n",
        "# ------------------------------------\n",
        "# ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯ Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ\n",
        "# ------------------------------------\n",
        "def add_polynomial_features(X, degree=2):\n",
        "    try:\n",
        "        top_features = X.iloc[:, :10]\n",
        "        poly = PolynomialFeatures(degree=degree, interaction_only=True, include_bias=False)\n",
        "        X_poly = poly.fit_transform(top_features)\n",
        "        poly_feature_names = poly.get_feature_names_out(top_features.columns)\n",
        "        df_poly = pd.DataFrame(X_poly, columns=poly_feature_names, index=X.index)\n",
        "        print(f\"â• ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: {df_poly.shape[1]}\")\n",
        "        return pd.concat([X, df_poly], axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ: {e}\")\n",
        "        return X\n",
        "\n",
        "# ------------------------------------\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´\n",
        "# ------------------------------------\n",
        "def full_preprocessing_pipeline():\n",
        "    print(\"ğŸš€ Ø´Ø±ÙˆØ¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´...\")\n",
        "    df = load_all_data()\n",
        "    df = remove_single_value_columns(df)\n",
        "    df = normalize(df)\n",
        "    df = replace_outliers_with_mean(df)\n",
        "\n",
        "    X = df.drop(columns=['label'])\n",
        "    y = df['label']\n",
        "\n",
        "    # Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ù‡Ù… Ø§ÙˆÙ„ÛŒÙ‡\n",
        "    X_selected, y = select_important_features(X, y, threshold=\"mean\")\n",
        "\n",
        "    # Ø§ÙØ²ÙˆØ¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯ Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ\n",
        "    X_extended = add_polynomial_features(X_selected)\n",
        "\n",
        "    # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¬Ø¯Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…\n",
        "    X_final, y = select_important_features(X_extended, y, threshold=\"mean\")\n",
        "\n",
        "    final_df = pd.concat([X_final, pd.Series(y, name='label')], axis=1)\n",
        "    print(\"âœ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯.\")\n",
        "    print(f\"ğŸ“Š Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡: {final_df.shape}\")\n",
        "    return final_df\n",
        "\n",
        "# ------------------------------------\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡\n",
        "# ------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    final_df = full_preprocessing_pipeline()\n",
        "final_df.to_csv(\"../data/FinalPreProcessing.csv\", index=False)\n",
        "print(\"ğŸ“ ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: FinalPreProcessing.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
